---
layout: post
title: 朴素贝叶斯
categories: ML
description: 朴素贝叶斯
---

# 总结
在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数分类算法，如决策树、KNN、逻辑回归、支持向量机等，它们都是**判别方法**，即直接学习出特征输出$Y$和特征$X$间的关系。(要么是决策函数$Y=f(X)$，要么是条件分布$P(Y|X)$)但朴素贝叶斯却是**生成方法**，即通过先验概率分布$P(Y)$和条件概率分布$P(X|Y)$，求出类别$Y$和特征$X$的联合分布$P(X,Y)=P(X|Y)P(Y)$，然后利用贝叶斯公式求出类别的后验概率分布$P(Y|X)$，概率最大的那个类别就是样本所属的类别。[$P\left( Y|X \right) =\frac{P\left( X,Y \right)}{P\left( X \right)}$]

## 相关假设
1. 特征间相互独立(朴素的由来)
2. 特征服从某种分布(伯努利分布/多项式分布/高斯分布)

## 数学描述
训练样本集如下：

$$(x_1^{(1)}, x_2^{(1)}, ...x_n^{(1)}, y_1), (x_1^{(2)}, x_2^{(2)}, ...x_n^{(2)},y_2), ... (x_1^{(m)}, x_2^{(m)}, ...x_n^{(m)}, y_n)$$﻿

共有$m$个样本，每个样本有$n$个特征，样本集中有$K$个类别，定义为${C_1,C_2,...,C_K}$，每个类别的样本个数分别为${m_1,m_2,...,m_K}$，其有$m_1+m_2+...+m_K=m$。若样本特征为离散值，则第$k$个类别第$j$个特征中第$l$个取值个数记为$m_{kjl}$。其中$$k\in\left\{ 1,2,\cdots ,K \right\} ,j\in\left\{ 1,2,\cdots ,n \right\}, l\in\left\{ 1,2,\cdots ,S_j \right\} $$， $S_j$表示第$j$个特征不同种类的取值数。由上述描述的训练集，预测$X^{\left( test \right)}$的类别

由训练样本集可得先验分布$$P(Y=C_k)(k=1,2,...K)$$及条件概率分布
$$P(X=x|Y=C_k) = P(X_1=x_1, X_2=x_2,...X_n=x_n|Y=C_k)$$
然后利用贝叶斯公式可得类别$Y$和特征$X$的联合分布$P(X,Y)$，定义如下：

$$P\left( X,Y=C_k \right)$$﻿
$$=P\left( Y=C_k \right) P\left( X=x|Y=C_k \right)$$
$$=P\left( Y=C_k \right) P\left( X_1=x_1,X_2=x_2,...X_n=x_n|Y=C_k \right)$$

由特征间的相互独立假设可知：
$$P(X_1=x_1, X_2=x_2,...X_n=x_n|Y=C_k)$$
$$=P(X_1=x_1|Y=C_k)P(X_2=x_2|Y=C_k)...P(X_n=x_n|Y=C_k)$$

由训练数据集求得$$P\left( X,Y=C_k \right)$$﻿后，即可根据贝叶斯定理，求得测试集中某个测试样本$\left( x_{1}^{\left( test \right)},x_{2}^{\left( test \right)},...x_{n}^{\left( test \right)} \right) $属于各个类别的后验概率，选择概率最大的类别作为测试样本的最终类别。
即

$$
C_{result}=\underset{C_k}{\underbrace{argmax}}P\left( Y=C_k|X=X^{\left( test \right)} \right) 
$$
$$
=\underset{C_k}{\underbrace{argmax}}P\left( X=X^{\left( test \right)}|Y=C_k \right) P\left( Y=C_k \right) /P\left( X=X^{\left( test \right)} \right) 
$$

又对于每一个样本来说，上述公式的分母是一样的，因此，预测公式可简化为

$$
C_{result} = \underbrace{argmax}_{C_k}P(X=X^{(test)}|Y=C_k)P(Y=C_k)
$$

再由朴素贝叶斯的独立性假设，进一步得：

$$
C_{result}=\underset{C_k}{\underbrace{argmax}}P\left( Y=C_k \right) \prod_{j=1}^n{P}\left( X_j=X_{j}^{\left( test \right)}|Y=C_k \right) 
$$


## 参数估计
由上述分析可知，只要求出$P(Y=C_k)和P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,...n)$，即可通过比较获得最终的分类结果。

对于$P(Y=C_k)$,若没有$Y$的先验概率，则通过极大似然估计计算$Y$的$K$个先验概率$P(Y=C_k)=m_k/m$，否则$P(Y=C_k)$为输入的先验概率

对于条件概率
$$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,...n)$$,
取决于我们的先验特征分布假设

更进一步地，对上述条件概率需先计算第$k$个类别的第$j$维特征的第$l$个取值的条件概率$P\left( X_j=x_{jl}^{(test)}|Y=C_k \right)$
- 如果$X_j$是离散的值，那么可以假设$X_j$符合多项式分布，此时有：
$$P\left( X_j=x_{jl}^{(test)}|Y=C_k \right) =\frac{m_{kjl}+\lambda}{m_k+S_j\lambda}$$﻿
    
$\lambda$为大于0的数字，当取1时，表示拉普拉斯平滑。(**主要防止出现为0的概率**)  

- 如果$X_j$是非常稀疏的离散值，即各个特征出现概率很低，则可以假设$X_j$符合伯努利分布，有：
$$P\left( X_j=x_{jl}^{(test)}|Y=C_k \right) =P\left( X_j|Y=C_k \right) x_{jl}^{(test)}+\left( 1-P\left( X_j|Y=C_k \right) \right) \left( 1-x_{jl}^{(test)} \right) $$

此时$l$只有两种取值 

- 如果$X_j$是连续值,则不需要计算各个$l$的取值概率，直接求正态分布的参数: 
$$P\left( X_j=x_j^{(test)}|Y=C_k \right) =\frac{1}{\sqrt{2\pi \sigma _{k}^{2}}}exp\text{(}-\frac{\left( x_j^{(test)}-\mu _k \right) ^2}{2\sigma _{k}^{2}}\text{)}$$

需要求出$\mu_k和\sigma_k^2$。 $\mu_k$为在样本类别$C_k$中所有$X_j$的平均值。$\sigma_k^2$为在样本类别$C_k$中所有$X_j$的方差

## 模型种类
根据输入特征分布的不同会有不同形式的朴素贝叶斯
### BernoulliNB
先验为伯努利分布的朴素贝叶斯。若样本特征是二元离散值或者很稀疏的多元离散值，则选用BernoulliNB
### MultinomialNB
先验为多项式分布的朴素贝叶斯。若样本特征的分布大部分是多元离散值，则选用MultinomialNB
### GaussianNB
先验为高斯分布的朴素贝叶斯。若样本特征的分布大部分是连续值，则选用GaussianNB

## 优缺点
优点：
1. 无复杂的计算，有稳定的分类效率
2. 对小规模的数据表现很好，可处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练
3. 对缺失数据不太敏感，算法也比较简单，常用于文本分类

缺点：
1. 由于朴素贝叶斯模型在给定输出类别的情况下,假设特征间相互独立，而这个假设在实际应用中往往是不成立的，在样本特征数比较多或者特征间相关性较大时，分类效果不好。而在特征相关性较小时，朴素贝叶斯性能较好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进
2. 需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳
3. 由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率
4. 对样本特征的表达形式很敏感

## 实战
### 垃圾邮件分类
* [spam.csv](https://github.com/carlos9310/carlos9310.github.io/tree/master/assets/data/data_spam/spam.csv)
* [NB_spam_detection.ipynb](https://github.com/carlos9310/carlos9310.github.io/tree/master/assets/source/NB_spam_detection.ipynb)

### 情感分类
* [stopwords.txt](https://github.com/carlos9310/carlos9310.github.io/tree/master/assets/data/data_sentiment/stopwords.txt)
* [test.combined.txt](https://github.com/carlos9310/carlos9310.github.io/tree/master/assets/data/data_sentiment/test.combined.txt)
* [train.negative.txt](https://github.com/carlos9310/carlos9310.github.io/tree/master/assets/data/data_sentiment/train.negative.txt)
* [train.positive.txt](https://github.com/carlos9310/carlos9310.github.io/tree/master/assets/data/data_sentiment/train.positive.txt)
* [NB_sentiment.ipynb](https://github.com/carlos9310/carlos9310.github.io/tree/master/assets/source/NB_sentiment.ipynb)

## 参考
* [朴素贝叶斯算法原理小结](https://www.cnblogs.com/pinard/p/6069267.html)
* [scikit-learn 朴素贝叶斯类库使用小结](https://www.cnblogs.com/pinard/p/6074222.html)
 

 